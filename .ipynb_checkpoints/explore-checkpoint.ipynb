{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "# cur = conn.cursor()\n",
    "# cur.execute('pragma table_info(Emails)')\n",
    "# f_1 = 'Id'\n",
    "# f_2 = 'MetadataTo'\n",
    "# f_3 = 'MetadataFrom'\n",
    "# f_4 = 'ExtractedSubject'\n",
    "# f_5 = 'ExtractedDateSent'\n",
    "# f_6 = 'ExtractedBodyText'\n",
    "# f_7 = 'RawText'\n",
    "# cur.execute('select {c1},{c2},{c3},{c4},{c5},{c6},{c7} from Emails'.\\\n",
    "#             format(c1=f_1,c2=f_2,c3=f_3,c4=f_4,c5=f_5,c6=f_6,c7=f_7))\n",
    "# cur.fetchall()\n",
    "# cur.fetchone()\n",
    "# columns_less = ['id','to','from','sub','date','body','raw']\n",
    "# columns = [str(tpl[1]) for tpl in columns]\n",
    "def clean_subject(sub):\n",
    "    email_type = None\n",
    "    if re.match('^[Ff][Ww]',sub)!=None:\n",
    "        email_type='fw'\n",
    "        sub = re.sub('^[Ff][Ww].*:\\s*(.+)',r'\\1',sub)\n",
    "    elif re.match('^[Rr][Ee]:',sub)!=None:\n",
    "        email_type='re'\n",
    "        sub = re.sub('^[Rr][Ee]:\\s*(.+)',r'\\1',sub)\n",
    "    elif re.match('^[Ff][Vv][Vv]:',sub)!=None:\n",
    "        email_type='fvv'\n",
    "        sub = re.sub('^[Ff][Vv][Vv]:\\s*(.+)',r'\\1',sub)\n",
    "    return email_type,sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sender</th>\n",
       "      <th>Sender_Id</th>\n",
       "      <th>Receiver</th>\n",
       "      <th>Receiver_Id</th>\n",
       "      <th>ExtractedDateSent</th>\n",
       "      <th>ExtractedSubject</th>\n",
       "      <th>email_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 10:16 AM</td>\n",
       "      <td>Wow</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>;H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday, September 12, 2012 11:52 AM</td>\n",
       "      <td>Chris Stevens</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12,2012 12:44 PM</td>\n",
       "      <td>Cairo Condemnation - Final</td>\n",
       "      <td>fvv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Russorv@state.gov</td>\n",
       "      <td>185.0</td>\n",
       "      <td>Wednesday, September 12, 2012 01:00 PM</td>\n",
       "      <td>Meet The Right Wing Extremist Behind Anti-Musl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 4:00 PM</td>\n",
       "      <td>Anti-Muslim film director in hiding, following...</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 6:08 PM</td>\n",
       "      <td>Secretary's remarks</td>\n",
       "      <td>fvv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Sunday, March 13, 2011 10:55 AM</td>\n",
       "      <td>AbZ and Hb3 on Libya and West Bank/Gaza</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12 2012 700 PM</td>\n",
       "      <td>hey</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>Anne-Marie Slaughter</td>\n",
       "      <td>10</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 04:33 PM</td>\n",
       "      <td>Not a dry eye in NEA</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>Harold Hongju Koh</td>\n",
       "      <td>77</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>Wendy Sherman</td>\n",
       "      <td>213</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 9:57 PM</td>\n",
       "      <td>The Youth of Libya</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>Wendy Sherman</td>\n",
       "      <td>213</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 10:03 PM</td>\n",
       "      <td>One More Photo</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12 2012 1026 PM</td>\n",
       "      <td>S today</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>Sherman, Wendy R</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>sullivanjj@state.gov</td>\n",
       "      <td>87.0</td>\n",
       "      <td></td>\n",
       "      <td>more on libya</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Russorv@state.gov</td>\n",
       "      <td>185.0</td>\n",
       "      <td></td>\n",
       "      <td>more on libya</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Russorv@state.gov</td>\n",
       "      <td>185.0</td>\n",
       "      <td>Wednesday, September 12, 2012 11:30 PM</td>\n",
       "      <td>Magariaf on attack on US in Libya. Sid</td>\n",
       "      <td>fw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                Sender  Sender_Id              Receiver  Receiver_Id  \\\n",
       "0    1         Jake Sullivan         87                     H         80.0   \n",
       "1    3          Cheryl Mills         32                    ;H          NaN   \n",
       "2    4          Cheryl Mills         32                     H         80.0   \n",
       "3    5       Hillary Clinton         80          Abedin, Huma          NaN   \n",
       "4    6       Hillary Clinton         80     Russorv@state.gov        185.0   \n",
       "5    7          Cheryl Mills         32                     H         80.0   \n",
       "6    8       Hillary Clinton         80          Abedin, Huma          NaN   \n",
       "7    9         Jake Sullivan         87                     H         80.0   \n",
       "8   11         Jake Sullivan         87                     H         80.0   \n",
       "9   13         Jake Sullivan         87                     H         80.0   \n",
       "10  14  Anne-Marie Slaughter         10                     H         80.0   \n",
       "11  15          Cheryl Mills         32                     H         80.0   \n",
       "12  16     Harold Hongju Koh         77                     H         80.0   \n",
       "13  17         Wendy Sherman        213                     H         80.0   \n",
       "14  18         Wendy Sherman        213                     H         80.0   \n",
       "15  19         Jake Sullivan         87                     H         80.0   \n",
       "16  20         Jake Sullivan         87      Sherman, Wendy R          NaN   \n",
       "17  21       Hillary Clinton         80  sullivanjj@state.gov         87.0   \n",
       "18  22       Hillary Clinton         80     Russorv@state.gov        185.0   \n",
       "19  23       Hillary Clinton         80     Russorv@state.gov        185.0   \n",
       "\n",
       "                         ExtractedDateSent  \\\n",
       "0   Wednesday, September 12, 2012 10:16 AM   \n",
       "1   Wednesday, September 12, 2012 11:52 AM   \n",
       "2    Wednesday, September 12,2012 12:44 PM   \n",
       "3                                            \n",
       "4   Wednesday, September 12, 2012 01:00 PM   \n",
       "5    Wednesday, September 12, 2012 4:00 PM   \n",
       "6                                            \n",
       "7    Wednesday, September 12, 2012 6:08 PM   \n",
       "8          Sunday, March 13, 2011 10:55 AM   \n",
       "9      Wednesday, September 12 2012 700 PM   \n",
       "10                                           \n",
       "11  Wednesday, September 12, 2012 04:33 PM   \n",
       "12                                           \n",
       "13   Wednesday, September 12, 2012 9:57 PM   \n",
       "14  Wednesday, September 12, 2012 10:03 PM   \n",
       "15    Wednesday, September 12 2012 1026 PM   \n",
       "16                                           \n",
       "17                                           \n",
       "18                                           \n",
       "19  Wednesday, September 12, 2012 11:30 PM   \n",
       "\n",
       "                                     ExtractedSubject email_type  \n",
       "0                                                 Wow         fw  \n",
       "1                                       Chris Stevens         re  \n",
       "2                          Cairo Condemnation - Final        fvv  \n",
       "3                                                           None  \n",
       "4   Meet The Right Wing Extremist Behind Anti-Musl...       None  \n",
       "5   Anti-Muslim film director in hiding, following...         fw  \n",
       "6                                                           None  \n",
       "7                                 Secretary's remarks        fvv  \n",
       "8             AbZ and Hb3 on Libya and West Bank/Gaza       None  \n",
       "9                                                 hey       None  \n",
       "10                                                          None  \n",
       "11                               Not a dry eye in NEA         re  \n",
       "12                                                          None  \n",
       "13                                 The Youth of Libya         fw  \n",
       "14                                     One More Photo         fw  \n",
       "15                                            S today         fw  \n",
       "16                                                          None  \n",
       "17                                      more on libya         fw  \n",
       "18                                      more on libya         fw  \n",
       "19             Magariaf on attack on US in Libya. Sid         fw  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "sql = \"\"\"select e.Id, p.Name Sender, e.SenderPersonId Sender_Id\n",
    ", e.MetadataTo Receiver, a.PersonId Receiver_Id, e.ExtractedDateSent, e.ExtractedSubject\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id \n",
    "left outer join Aliases a on lower(e.MetadataTo)=a.Alias\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "temp = emails.ExtractedSubject.apply(clean_subject)\n",
    "emails['ExtractedSubject'] = [tpl[1] for tpl in temp]\n",
    "emails['email_type'] = email_type = [tpl[0] for tpl in temp]\n",
    "emails.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up sender receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NimishaAg\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded emails:  174\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sender</th>\n",
       "      <th>Sender_Id</th>\n",
       "      <th>Receiver</th>\n",
       "      <th>Receiver_Id</th>\n",
       "      <th>ExtractedDateSent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 10:16 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12,2012 12:44 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>81.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Russorv@state.gov</td>\n",
       "      <td>185.0</td>\n",
       "      <td>Wednesday, September 12, 2012 01:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 4:00 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Abedin, Huma</td>\n",
       "      <td>81.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 6:08 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Sunday, March 13, 2011 10:55 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12 2012 700 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14</td>\n",
       "      <td>Anne-Marie Slaughter</td>\n",
       "      <td>10</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>Cheryl Mills</td>\n",
       "      <td>32</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 04:33 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>Harold Hongju Koh</td>\n",
       "      <td>77</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17</td>\n",
       "      <td>Wendy Sherman</td>\n",
       "      <td>213</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 9:57 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>Wendy Sherman</td>\n",
       "      <td>213</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12, 2012 10:03 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>H</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Wednesday, September 12 2012 1026 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>Jake Sullivan</td>\n",
       "      <td>87</td>\n",
       "      <td>Sherman, Wendy R</td>\n",
       "      <td>213.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>21</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>sullivanjj@state.gov</td>\n",
       "      <td>87.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Russorv@state.gov</td>\n",
       "      <td>185.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>23</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>Russorv@state.gov</td>\n",
       "      <td>185.0</td>\n",
       "      <td>Wednesday, September 12, 2012 11:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>80</td>\n",
       "      <td>sullivanjj@state.gov</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Wednesday, September 12, 2012 11:30 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                Sender  Sender_Id              Receiver  Receiver_Id  \\\n",
       "0    1         Jake Sullivan         87                     H         80.0   \n",
       "2    4          Cheryl Mills         32                     H         80.0   \n",
       "3    5       Hillary Clinton         80          Abedin, Huma         81.0   \n",
       "4    6       Hillary Clinton         80     Russorv@state.gov        185.0   \n",
       "5    7          Cheryl Mills         32                     H         80.0   \n",
       "6    8       Hillary Clinton         80          Abedin, Huma         81.0   \n",
       "7    9         Jake Sullivan         87                     H         80.0   \n",
       "8   11         Jake Sullivan         87                     H         80.0   \n",
       "9   13         Jake Sullivan         87                     H         80.0   \n",
       "10  14  Anne-Marie Slaughter         10                     H         80.0   \n",
       "11  15          Cheryl Mills         32                     H         80.0   \n",
       "12  16     Harold Hongju Koh         77                     H         80.0   \n",
       "13  17         Wendy Sherman        213                     H         80.0   \n",
       "14  18         Wendy Sherman        213                     H         80.0   \n",
       "15  19         Jake Sullivan         87                     H         80.0   \n",
       "16  20         Jake Sullivan         87      Sherman, Wendy R        213.0   \n",
       "17  21       Hillary Clinton         80  sullivanjj@state.gov         87.0   \n",
       "18  22       Hillary Clinton         80     Russorv@state.gov        185.0   \n",
       "19  23       Hillary Clinton         80     Russorv@state.gov        185.0   \n",
       "20  24       Hillary Clinton         80  sullivanjj@state.gov         87.0   \n",
       "\n",
       "                         ExtractedDateSent  \n",
       "0   Wednesday, September 12, 2012 10:16 AM  \n",
       "2    Wednesday, September 12,2012 12:44 PM  \n",
       "3                                           \n",
       "4   Wednesday, September 12, 2012 01:00 PM  \n",
       "5    Wednesday, September 12, 2012 4:00 PM  \n",
       "6                                           \n",
       "7    Wednesday, September 12, 2012 6:08 PM  \n",
       "8          Sunday, March 13, 2011 10:55 AM  \n",
       "9      Wednesday, September 12 2012 700 PM  \n",
       "10                                          \n",
       "11  Wednesday, September 12, 2012 04:33 PM  \n",
       "12                                          \n",
       "13   Wednesday, September 12, 2012 9:57 PM  \n",
       "14  Wednesday, September 12, 2012 10:03 PM  \n",
       "15    Wednesday, September 12 2012 1026 PM  \n",
       "16                                          \n",
       "17                                          \n",
       "18                                          \n",
       "19  Wednesday, September 12, 2012 11:30 PM  \n",
       "20  Wednesday, September 12, 2012 11:30 PM  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch data in pandas dataframe\n",
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "# sql = \"select * from Emails\"\n",
    "# emails = pd.read_sql_query(sql,conn)\n",
    "# columns = ['Id','SenderPersonId','MetadataFrom','MetadataTo','ExtractedDateSent','ExtractedSubject','ExtractedBodyText']\n",
    "# emails[columns].head(5)\n",
    "# emails[emails['SenderPersonId']==80][columns].head(5)\n",
    "# x = emails.groupby('SenderPersonId').size()\n",
    "# print 'No of mails sent by HC =',x[80]\n",
    "\n",
    "# Assign receiver Id\n",
    "sql = \"\"\"select e.Id, p.Name Sender, \n",
    "e.SenderPersonId Sender_Id, e.MetadataTo Receiver, a.PersonId Receiver_Id, e.ExtractedDateSent\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id \n",
    "left outer join Aliases a on lower(e.MetadataTo)=a.Alias\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "# emails_noNA = emails.dropna(how='any')\n",
    "# emails.head(20)\n",
    "idx_NaN = [i for i,val in enumerate(emails['Receiver_Id']) if np.isnan(val)]\n",
    "\n",
    "\n",
    "# Create dictionary personId:name\n",
    "sql = \"select * from Persons\"\n",
    "persons = pd.read_sql_query(sql,conn)\n",
    "persons_dict = {}\n",
    "for entry in persons.itertuples():\n",
    "    persons_dict[entry[2]]=entry[1]\n",
    "    \n",
    "# Fill up missing Receiver_Id fields\n",
    "receiver_dict = {}\n",
    "pattern = re.compile('^(\\w+),\\sp*(\\w+)')\n",
    "for idx in idx_NaN:\n",
    "    name = emails['Receiver'][idx]\n",
    "    pat_obj = pattern.match(name)\n",
    "    if pat_obj!=None:\n",
    "        person_name = pat_obj.group(2)+' '+pat_obj.group(1)\n",
    "        try:\n",
    "            person_id = persons_dict[person_name]\n",
    "            emails['Receiver_Id'][idx]=person_id\n",
    "        except:\n",
    "            pass\n",
    "temp = emails.shape[0]\n",
    "emails = emails.dropna(how='any')\n",
    "print('Discarded emails: ',temp-emails.shape[0])\n",
    "\n",
    "emails.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# txt = 'Thursday, September 30, 2010 9:04 PM'\n",
    "# txt = 'Thu Sep 17 06:03:43 2009'\n",
    "def clean_date_aux(ts):\n",
    "    pat = re.compile('\\'?(?P<weekday>\\w+),?\\s*(?P<month>\\w+) (?P<date>\\d+),?\\s*(?P<year>\\d+) (?P<time>.+) (?P<ampm>[A|P]M)')\n",
    "    pat1 = re.compile('(?P<weekday>.+) (?P<month>.+) (?P<date>.+) (?P<time>.+) (?P<year>.+)')\n",
    "    if ts=='':\n",
    "        return np.NAN\n",
    "    re_obj = None\n",
    "    try:\n",
    "        if pat.match(ts)!=None:\n",
    "            re_obj = pat.match(ts)\n",
    "        elif pat1.match(ts)!=None:\n",
    "            re_obj = pat1.match(ts)\n",
    "        else:\n",
    "            raise\n",
    "        # Process date\n",
    "        month = re_obj.group('month')\n",
    "        day = re_obj.group('date')\n",
    "        year = re_obj.group('year')\n",
    "        time = re_obj.group('time').replace('.',':')\n",
    "        # Check for numeric value of day and year\n",
    "        try:\n",
    "            month = month[:3]\n",
    "            day = int(day)\n",
    "            year = int(year)\n",
    "        except:\n",
    "            raise\n",
    "        # Check time string\n",
    "        if ':' not in time and len(time)>2:\n",
    "            time = time[:-2]+':'+time[-2:]\n",
    "        try:\n",
    "            time = time + ' ' + re_obj.group('ampm')\n",
    "        except:\n",
    "            pass\n",
    "        ts_str = \"{} {} {} {}\".format(month,day,year,time)\n",
    "        try:\n",
    "            pd_ts = pd.to_datetime(ts_str)\n",
    "            return pd_ts\n",
    "        except:\n",
    "            raise\n",
    "    except:\n",
    "        return np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_date(raw_date):\n",
    "    return clean_date_aux(raw_date)\n",
    "emails['Date']=emails.ExtractedDateSent.apply(clean_date)\n",
    "emails.head(10)\n",
    "# emails = emails.dropna(how='any')\n",
    "emails.index = emails.Date\n",
    "emails.sort_index(inplace=True)\n",
    "mindate,maxdate = emails.index.min(),emails.index.max()\n",
    "\"Date Range from {} to {}\".format(mindate.date(),maxdate.date())\n",
    "emails[:'2011-01'].Id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process email body: Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HillaryEmails(object):\n",
    "    def __init__(self,doc_vectors):\n",
    "        self.doc_vectors = doc_vectors\n",
    "    def __iter__(self):\n",
    "        for vector in self.doc_vectors:\n",
    "            yield vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(26189 unique tokens: ['automotive', 'creation', 'rectangular', 'sheets', 'acct']...) from 7788 documents (total 301742 corpus positions)\n",
      "INFO : storing corpus in Matrix Market format to emails_bow.mm\n",
      "INFO : saving sparse matrix to emails_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 329 ms\n",
      "[(1, 1), (2, 2), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : saved 7788x26189 matrix, density=0.099% (201304/203959932)\n",
      "INFO : saving MmCorpus index to emails_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "def do_BoW(tokens):\n",
    "    return vocab.doc2bow(tokens)\n",
    "def get_maxwords(vector):\n",
    "    minimum = int(len(vector)*0.2)\n",
    "    minimum = min(10,minimum)\n",
    "    global vocab\n",
    "    temp = sorted(vector,key=lambda x:x[1],reverse=True)\n",
    "    topic = [vocab[it[0]] for it in temp[:minimum]]\n",
    "    return topic\n",
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "sql = \"\"\"select e.Id, p.Name Sender, \n",
    "e.SenderPersonId Sender_Id, e.ExtractedDateSent, e.ExtractedBodyText\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "#print(emails)\n",
    "emails['ExtractedDateSent']=emails.ExtractedDateSent.apply(clean_date)\n",
    "#print(emails['ExtractedDateSent'])\n",
    "# Step 1: Mails from Hillary\n",
    "#emails = emails[emails['Sender_Id']==80]\n",
    "#print(emails)\n",
    "# Step 2: Tokenize email body\n",
    "email_tokens = emails.ExtractedBodyText.apply(tokenize)\n",
    "#print(email_tokens)\n",
    "doc_stream = [tokens for tokens in email_tokens]\n",
    "#print(doc_stream)\n",
    "print(len(doc_stream))\n",
    "# Step 3: Build Vocabulary\n",
    "%time vocab = gensim.corpora.Dictionary(doc_stream)\n",
    "#print(vocab)\n",
    "    # print vocab.items()\n",
    "    # vocab.filter_extremes(no_below=20,no_above=0.8)\n",
    "    # vocab.filter_extremes(no_above=0.5)\n",
    "# Step 4: Run BoW on docs\n",
    "vectors_doc = email_tokens.apply(do_BoW)\n",
    "print(vectors_doc[3])\n",
    "maxwords = vectors_doc.apply(get_maxwords)\n",
    "#print(\"max\\n\",maxwords)\n",
    "#Step 5: save doc vectors in mm file\n",
    "hillary_emails_corpus = HillaryEmails(vectors_doc)\n",
    "#print(hillary_emails_corpus)\n",
    "%time gensim.corpora.MmCorpus.serialize('emails_bow.mm',hillary_emails_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : calculating IDF weights for 7788 documents and 26188 features (201304 matrix non-zeros)\n",
      "INFO : using symmetric alpha at 0.01\n",
      "INFO : using symmetric eta at 0.01\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 100 topics, 1 passes over the supplied corpus of 7788 documents, updating model once every 2000 documents, evaluating perplexity every 7788 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : PROGRESS: pass 0, at document #2000/7788\n",
      "INFO : merging changes from 2000 documents into a model of 7788 documents\n",
      "INFO : topic #58 (0.010): 0.008*pm + 0.008*pls + 0.007*calling + 0.006*email + 0.006*donna + 0.005*choose + 0.005*messag + 0.005*option + 0.005*lauren + 0.004*send\n",
      "INFO : topic #42 (0.010): 0.008*libya + 0.008*acting + 0.007*sunday + 0.007*facebooks + 0.006*esther + 0.006*talk + 0.005*sullivanjj + 0.005*gov + 0.005*state + 0.005*aq\n",
      "INFO : topic #57 (0.010): 0.012*thx + 0.008*thank + 0.008*mention + 0.008*feedback + 0.008*alal + 0.006*fine + 0.006*mtg + 0.006*podesta + 0.006*henriette + 0.006*kolb\n",
      "INFO : topic #11 (0.010): 0.008*angle + 0.007*turner + 0.006*know + 0.006*home + 0.006*upstairs + 0.005*thx + 0.005*room + 0.005*spent + 0.005*mitchell + 0.005*review\n",
      "INFO : topic #3 (0.010): 0.034*ok + 0.022*fyi + 0.007*fm + 0.007*thx + 0.006*pis + 0.006*yang + 0.006*pm + 0.005*importance + 0.005*courier + 0.005*today\n",
      "INFO : topic diff=94.816987, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #4000/7788\n",
      "INFO : merging changes from 2000 documents into a model of 7788 documents\n",
      "INFO : topic #57 (0.010): 0.013*podesta + 0.008*drive + 0.007*edits + 0.006*thank + 0.006*service + 0.006*sarah + 0.006*fun + 0.006*night + 0.005*thx + 0.005*fine\n",
      "INFO : topic #59 (0.010): 0.019*china + 0.014*september + 0.013*saw + 0.010*december + 0.010*revised + 0.009*willing + 0.009*hillary + 0.008*printed + 0.008*deal + 0.006*worry\n",
      "INFO : topic #19 (0.010): 0.021*fyi + 0.016*paper + 0.013*hold + 0.011*lift + 0.011*vermarr + 0.009*nomination + 0.008*ambassadors + 0.007*cloakroom + 0.007*came + 0.006*print\n",
      "INFO : topic #25 (0.010): 0.021*supposed + 0.012*hello + 0.011*details + 0.010*wouldn + 0.008*able + 0.008*haiti + 0.007*won + 0.007*waiting + 0.007*including + 0.006*ok\n",
      "INFO : topic #74 (0.010): 0.017*ok + 0.015*twice + 0.015*talk + 0.014*know + 0.013*oprah + 0.012*added + 0.008*let + 0.008*highlight + 0.008*visa + 0.008*rch\n",
      "INFO : topic diff=1.536310, rho=0.707107\n",
      "INFO : PROGRESS: pass 0, at document #6000/7788\n",
      "INFO : merging changes from 2000 documents into a model of 7788 documents\n",
      "INFO : topic #24 (0.010): 0.023*safe + 0.022*travels + 0.013*bring + 0.012*point + 0.011*november + 0.011*plan + 0.009*afp + 0.009*discussing + 0.007*consider + 0.006*nite\n",
      "INFO : topic #41 (0.010): 0.045*january + 0.021*okay + 0.017*called + 0.011*friend + 0.009*hotel + 0.009*cut + 0.009*couple + 0.009*opening + 0.007*explain + 0.006*scanned\n",
      "INFO : topic #13 (0.010): 0.043*yes + 0.021*jack + 0.019*attached + 0.017*mashabane + 0.015*faxed + 0.015*fm + 0.011*rec + 0.010*russia + 0.009*announced + 0.005*excited\n",
      "INFO : topic #7 (0.010): 0.053*importance + 0.046*high + 0.011*labour + 0.011*reports + 0.010*peter + 0.010*guardian + 0.010*fyi + 0.008*parties + 0.008*left + 0.007*polls\n",
      "INFO : topic #32 (0.010): 0.019*minister + 0.015*foreign + 0.011*russian + 0.010*personal + 0.008*tomorrow + 0.008*frattini + 0.008*info + 0.008*staying + 0.007*vp + 0.007*greece\n",
      "INFO : topic diff=1.753651, rho=0.577350\n",
      "INFO : -33.557 per-word bound, 12638842110.3 perplexity estimate based on a held-out corpus of 1788 documents with 4428 words\n",
      "INFO : PROGRESS: pass 0, at document #7788/7788\n",
      "INFO : merging changes from 1788 documents into a model of 7788 documents\n",
      "INFO : topic #42 (0.010): 0.014*share + 0.011*advice + 0.010*meetings + 0.008*table + 0.008*esther + 0.005*incredibly + 0.004*news + 0.004*arabia + 0.004*understands + 0.004*khartoum\n",
      "INFO : topic #97 (0.010): 0.035*sullivanjj + 0.029*gov + 0.026*sullivan + 0.023*state + 0.021*jacob + 0.019*pm + 0.019*sunday + 0.014*abedin + 0.014*huma + 0.013*asking\n",
      "INFO : topic #56 (0.010): 0.530*fyi + 0.016*anytime + 0.007*law + 0.007*goy + 0.005*talk + 0.004*apologize + 0.004*tough + 0.004*ahead + 0.004*surprised + 0.003*paymasters\n",
      "INFO : topic #33 (0.010): 0.025*hrod + 0.024*clintonemail + 0.020*com + 0.012*memorial + 0.010*welcome + 0.010*rest + 0.008*das + 0.007*gis + 0.006*class + 0.006*saturday\n",
      "INFO : topic #12 (0.010): 0.037*connect + 0.016*tax + 0.013*talk + 0.012*yeah + 0.008*assist + 0.008*comment + 0.007*missile + 0.007*necessary + 0.006*today + 0.005*scott\n",
      "INFO : topic diff=1.146796, rho=0.500000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, 0.032399544459246581),\n",
      " (15, 0.13140373535887201),\n",
      " (48, 0.10800230312143999),\n",
      " (50, 0.030049489861467445),\n",
      " (51, 0.029140269637242703),\n",
      " (58, 0.03951807950664124),\n",
      " (60, 0.017918430507101343),\n",
      " (68, 0.033181294291386242),\n",
      " (69, 0.22188407072877964),\n",
      " (76, 0.198957820138216),\n",
      " (96, 0.021149013271515055),\n",
      " (97, 0.016249481525258784)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "tfidf = models.TfidfModel(vectors_doc)\n",
    "#print(tfidf)\n",
    "corpus_tfidf = tfidf[vectors_doc]\n",
    "#for doc in corpus_tfidf:\n",
    "#    print(doc)\n",
    "model = models.LdaModel(corpus_tfidf, id2word=vocab, num_topics=100)\n",
    "corpus_lda = model[corpus_tfidf]\n",
    "from pprint import pprint\n",
    "#pprint(model.print_topics(-1))\n",
    "pprint(model.get_document_topics(vectors_doc[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2139, 0.028773432544645601), (2642, 0.015492575342382223), (322, 0.014463027824833148), (796, 0.014114387678011618), (1163, 0.012637920628845355), (156, 0.0092520014076107431), (3957, 0.0092361998007042413), (248, 0.0091865123111745418), (312, 0.0068661560572628941), (409, 0.0059183739295926158), (357, 0.0057744852755316298), (6177, 0.0043459404342122716), (874, 0.0043259935966280675), (1747, 0.0040682952382727006), (378, 0.0040361981132405519), (215, 0.0040118835243991065), (7968, 0.0038322089963970609), (5660, 0.0037777170684219134), (87, 0.0032040389555421504), (179, 0.003171790585571618), (78, 0.0031316601717658165), (4885, 0.0029148626094638568), (170, 0.0027108328227575594), (233, 0.0025102084648676659), (400, 0.0024329232985438329), (139, 0.002426084361283035), (224, 0.0024112756454794004), (20351, 0.0023754912472413198), (2745, 0.0023541155670572926), (371, 0.0023341244871886175), (123, 0.0023025929567070044), (4080, 0.0022622092382931572), (355, 0.002258219825829065), (135, 0.0020902515405377977), (380, 0.0020148191023364051), (15458, 0.0019942688389729402), (9125, 0.0019355734626659632), (329, 0.0018403338253154644), (1860, 0.0018010568576049886), (4843, 0.0017994591109494022), (541, 0.0017560777887203839), (464, 0.0017367934193789514), (795, 0.001733427218279279), (1098, 0.0017046263349675566), (127, 0.0016610254472146327), (342, 0.0015596075666179327), (274, 0.0015536549580121286), (222, 0.0015455102999445136), (149, 0.0015363720060595578), (1776, 0.0015066697877712291)]\n",
      "\n",
      "checking\n",
      "readout\n",
      "flight\n",
      "quick\n",
      "meant\n",
      "coalition\n",
      "reaching\n",
      "arab\n",
      "board\n",
      "imagine\n",
      "strategic\n",
      "legislation\n",
      "jake\n",
      "dinner\n",
      "weighed\n",
      "week\n",
      "negotiating\n",
      "solis\n",
      "arguments\n",
      "negotiated\n",
      "clark\n",
      "urge\n",
      "secretary\n",
      "compromise\n",
      "arms\n",
      "middle\n",
      "bombing\n",
      "lds\n",
      "leading\n",
      "east\n",
      "renew\n",
      "cdm\n",
      "jazeera\n",
      "promises\n",
      "thoughtful\n",
      "jury\n",
      "voinovich\n",
      "debated\n",
      "tuesday\n",
      "rich\n",
      "effort\n",
      "victoria\n",
      "calls\n",
      "talked\n",
      "important\n",
      "push\n",
      "league\n",
      "desire\n",
      "expect\n",
      "hours\n",
      "\n",
      "Anne-Marie Slaughter\n",
      "Sunday, March 13, 2011 9:39 PM\n",
      "Jacob Mills, Cheryl D; Reinesp@stategov; Abedin, Hurtle\n",
      "piece on libya nfz that will appear on NYT op-ed page tomorrow\n",
      "B6\n",
      "wanted to make sure you had a heads up. AM\n",
      "Anne-Marie Slaughter is a professor of politics and international affairs at Princeton.\n",
      "By Anne-Marie Slaughter\n",
      "PRESIDENT Obama says the noose is tightening around Col. Muammar al-Qaddafi. In fact, it is\n",
      "tightening around the Libyan rebels, as Colonel Qaddafi makes the most of the world's dithering and\n",
      "steadily retakes rebel-held tow.ns, The United States and Europe are temporizing on a no-flight zone\n",
      "while the Organization of the Islamic Conference, the Gulf Cooperation Council and now the Arab\n",
      "League have all called on the United Nations Security Council to authorize one. Opponents of a no-\n",
      "flight zone have put forth five main arguments, none of which, on closer examination, hold up.\n",
      "It's not in our interest. Gen. Wesley K. Clark argues that \"Libya doesn't sell much oil to the United\n",
      "States\" and that while Americans \"want to support democratic movements in the region,\" they are\n",
      "already doing that in Iraq and Afghanistan. Framing this issue in terms of oil is exactly what Arab\n",
      "populations and indeed much of the world expect, which is why they are so cynical about our\n",
      "professions of support for democracy and human rights. Now we have a chance to support a new\n",
      "beginning in the Muslim world — a new beginning of accountable governments that can provide\n",
      "services and opportunities for their citizens in ways that could dramatically decrease support for\n",
      "terrorist groups and violent extremism. It's hard to imagine something more in our strategic interest.\n",
      "It will be counter-productive. Many thoughtful commentators, including Al Jazeera's director\n",
      "general, Wadah Khanfar, argue that what is most important about the Arab spring is that it is coming\n",
      "from Arabs themselves. From this perspective, Western military intervention will play right into\n",
      "Qaddati's hands, allowing him to broadcast pictures of Western bombs falling on Arab civilians. But\n",
      "these arguments, while important, must be weighed against the appeals of of Libyan opposition\n",
      "fighters asking for international help, and now, astonishingly, against support for a no-flight zone by\n",
      "some of the same governments that have kept their populations quiescent by holding up the specter of\n",
      "foreign intervention. Assuming that a no-flight zone can be imposed by an international coalition that\n",
      "includes Arab states, we have an opportunity to establish a new narrative of Western support for Arab\n",
      "democrats.\n",
      "U.S. Department of State\n",
      "Case No. F-2015-04841\n",
      "Doc No. C05739568\n",
      "Date: 05/13/2015\n",
      "STATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\n",
      "SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-SCB0045012\n",
      "U.S. Department of State\n",
      "Case No. F-2015-04841\n",
      "Doc No. C05739568\n",
      "Date: 05/13/2015\n",
      "STATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\n",
      "SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER.\n",
      "It won't work. The United States ambassador to NATO, Ivo H. Daaider, argues that stopping\n",
      "Colonel Qaddafi's air force will not be decisive; he will continue to inflict damage with tanks and\n",
      "helicopters, bombing oil refineries and depots on his way to retaking key towns. But the potential\n",
      "effect of a no-flight zone must also be assessed in terms of Colonel Qaddafi's own calculations about\n",
      "his future. Richard Downie of the Center for Strategic and International Studies argues that although\n",
      "Colonel Qaddafi cultivates a mad-dictator image, he has been a canny survivor and political\n",
      "manipulator for 40 years. He is aware of debates with regard to a no-flight zone and is timing his\n",
      "military campaign accordingly; he is also capable of using his air force just enough to gain strategic\n",
      "advantage, but not enough to trigger a no-flight zone. If the international community lines up against\n",
      "him and is willing to crater his runways and take out his antiaircraft weapons, he might well renew his\n",
      "offer of a negotiated departure.\n",
      "If it does work, we don't know what we will get. Revolutions are almost always followed by internal\n",
      "divisions among the revolutionaries. We should not expect a rosy, Jeffersonian Libya. But the choice\n",
      "is between uncertainty and the certainty that if Colonel Qaddafi wins, regimes acrosS the region will\n",
      "conclude that force is the livay to answer protests. And when Colonel Qaddafi massacres the\n",
      "opposition, young protesters across the Middle East will conclude that when we were asked to support\n",
      "their cause with more than words, we blinked. Americans in turn will read the words of Mr. Obama's\n",
      "June 2009 speech in Cairo., with its lofty promises to stand for universal human rights, and cringe.\n",
      "Let's arm the rebels instead. Some commentators who agree with the analysis above say we could\n",
      "'better accomplish our goals by providing intelligence and arms to the opposition. That would, of\n",
      "course, be much easier for us. It undoubtedly appeals to Mn Obama as a neat compromise between\n",
      "the desire to help the protesters and the desire not to overrule his defense secretary's reluctance to\n",
      "participate in a no-flight zone. But we would be providing arms not to a disciplined military, but to\n",
      "ragged groups of brave volunteers who barely know how to use the weapons they have. They need\n",
      "action that will change the situation on. the ground for Colonel Qaddafi, as well as his the calculations.\n",
      "Moreover, by the time arms and intelligence could take effect, it is quite likely that Colonel Qaddafi\n",
      "will have retaken or at least besieged .Benghazi, the opposition stronghold.\n",
      "The United States should immediately ask the Security Council to authorize a no-flight zOne and\n",
      "make clear to Russia and China that if they block the resolution, the blood of the Libyan opposition\n",
      "will be on their hands. We should push them at least to abstain and bring the issue to a vote as soon as\n",
      "possible. If we get a resolution, we should work with the Arab League to quickly assemble an\n",
      "international coalition to impose the no-flight zone. If the Security Council fails toact, then we should\n",
      "recognize the opposition Interim National Council as the legitimate Libyan government, as France has\n",
      "done, and work with the Arab League to give the council any assistance it requests:\n",
      "U.S. Department of State\n",
      "Case No. F-2015-04841\n",
      "Doc No. C05739568\n",
      "Date: 05/13/2015\n",
      "STATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\n",
      "SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-SCB0045013\n",
      "U.S. Department of State\n",
      "Case No. F-2015-04841\n",
      "Doc No. C05739568\n",
      "Date: 05/13/2015\n",
      "STATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\n",
      "SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER.\n",
      "Any use of force must be carefully and fully debated, but that debate has now been had. Ifs been\n",
      "raging for a week, during which almost every Arab country has come on board calling for a no-flight\n",
      "zone and Colonel Qaddafi has retaken two key cities and is moving toward three more. It is time to\n",
      "act. $\n",
      "U.S. Department of State\n",
      "Case No. F-2015-04841\n",
      "Doc No. C05739568\n",
      "Date: 05/13/2015\n",
      "STATE DEPT. - PRODUCED TO HOUSE SELECT BENGHAZI COMM.\n",
      "SUBJECT TO AGREEMENT ON SENSITIVE INFORMATION & REDACTIONS. NO FOIA WAIVER. STATE-5CB0045014\n"
     ]
    }
   ],
   "source": [
    "topic = 69\n",
    "print(model.get_topic_terms(topic, topn=50))\n",
    "print()\n",
    "indices = [ind for ind,_ in model.get_topic_terms(topic, topn=50)]\n",
    "for i in indices:\n",
    "    print(vocab[i])\n",
    "print()\n",
    "print(emails.ExtractedBodyText.get(10))\n",
    "#print(model.get_term_topics(vocab.token2id('state')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from emails_bow.mm.index\n",
      "INFO : initializing corpus reader from emails_bow.mm\n",
      "INFO : accepted corpus with 1993 documents, 4249 features, 15844 non-zero entries\n",
      "INFO : using symmetric alpha at 0.05\n",
      "INFO : using symmetric eta at 0.05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(1993 documents, 4249 features, 15844 non-zero entries)\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : input corpus stream has no len(); counting documents\n",
      "INFO : running online LDA training, 20 topics, 4 passes over the supplied corpus of 1600 documents, updating model once every 1600 documents, evaluating perplexity every 1600 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : -20.086 per-word bound, 1113037.4 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 0, at document #1600/1600\n",
      "INFO : topic #9 (0.050): 0.013*com + 0.011*print + 0.010*hrod + 0.009*clintonemail + 0.008*know + 0.008*state + 0.008*pis + 0.007*pls + 0.007*pm + 0.006*time\n",
      "INFO : topic #1 (0.050): 0.016*state + 0.011*tomorrow + 0.010*let + 0.009*com + 0.008*hrod + 0.008*gov + 0.007*time + 0.007*clintonemail + 0.007*thx + 0.006*pls\n",
      "INFO : topic #19 (0.050): 0.026*pls + 0.016*print + 0.016*ok + 0.013*know + 0.008*need + 0.008*let + 0.008*time + 0.007*set + 0.007*send + 0.005*holbrooke\n",
      "INFO : topic #11 (0.050): 0.017*pls + 0.013*state + 0.011*ok + 0.010*com + 0.010*list + 0.009*let + 0.009*hrod + 0.008*add + 0.008*want + 0.008*clintonemail\n",
      "INFO : topic #16 (0.050): 0.016*ok + 0.009*best + 0.007*year + 0.007*forward + 0.006*thanks + 0.006*hillary + 0.006*let + 0.006*like + 0.006*know + 0.006*tomorrow\n",
      "INFO : topic diff=13.809025, rho=1.000000\n",
      "INFO : -10.309 per-word bound, 1268.5 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 1, at document #1600/1600\n",
      "INFO : topic #17 (0.050): 0.032*pls + 0.026*print + 0.012*state + 0.011*com + 0.010*hrod + 0.010*clintonemail + 0.009*pm + 0.007*way + 0.007*gov + 0.006*monday\n",
      "INFO : topic #10 (0.050): 0.024*know + 0.016*thx + 0.012*time + 0.011*let + 0.011*http + 0.010*com + 0.008*home + 0.008*want + 0.006*talk + 0.006*free\n",
      "INFO : topic #13 (0.050): 0.038*state + 0.016*pm + 0.014*gov + 0.012*department + 0.010*benghazi + 0.010*house + 0.010*subject + 0.009*pis + 0.008*time + 0.007*dept\n",
      "INFO : topic #5 (0.050): 0.021*state + 0.018*gov + 0.015*thx + 0.012*hrod + 0.010*send + 0.010*com + 0.010*clintonemail + 0.010*pm + 0.009*fw + 0.009*letter\n",
      "INFO : topic #0 (0.050): 0.013*state + 0.011*pls + 0.009*told + 0.008*like + 0.007*lewis + 0.007*work + 0.006*know + 0.006*thank + 0.006*com + 0.006*list\n",
      "INFO : topic diff=1.002656, rho=0.577350\n",
      "INFO : -9.759 per-word bound, 866.8 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 2, at document #1600/1600\n",
      "INFO : topic #7 (0.050): 0.012*thx + 0.012*tomorrow + 0.011*release + 0.010*dc + 0.010*asked + 0.009*work + 0.009*good + 0.008*pls + 0.008*come + 0.008*know\n",
      "INFO : topic #16 (0.050): 0.014*ok + 0.012*forward + 0.012*best + 0.011*year + 0.009*agree + 0.009*thanks + 0.008*coming + 0.008*new + 0.007*tomorrow + 0.007*know\n",
      "INFO : topic #19 (0.050): 0.018*pls + 0.012*parlak + 0.011*know + 0.008*let + 0.008*print + 0.007*set + 0.007*immigration + 0.006*later + 0.006*need + 0.005*judith\n",
      "INFO : topic #8 (0.050): 0.060*ok + 0.056*state + 0.055*gov + 0.032*com + 0.031*clintonemail + 0.030*hrod + 0.015*pm + 0.011*wednesday + 0.011*sullivanjj + 0.010*thx\n",
      "INFO : topic #13 (0.050): 0.043*state + 0.017*pm + 0.015*department + 0.013*benghazi + 0.012*house + 0.012*gov + 0.012*subject + 0.010*dept + 0.009*time + 0.009*produced\n",
      "INFO : topic diff=0.688886, rho=0.500000\n",
      "INFO : -9.542 per-word bound, 745.3 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 3, at document #1600/1600\n",
      "INFO : topic #15 (0.050): 0.029*thx + 0.021*yes + 0.019*tomorrow + 0.018*talk + 0.012*let + 0.011*early + 0.009*today + 0.009*know + 0.008*meet + 0.008*ready\n",
      "INFO : topic #9 (0.050): 0.011*com + 0.011*know + 0.008*want + 0.008*hrod + 0.007*clintonemail + 0.007*time + 0.007*pis + 0.007*dc + 0.006*let + 0.006*mentioned\n",
      "INFO : topic #5 (0.050): 0.018*state + 0.015*gov + 0.014*thx + 0.013*send + 0.010*letter + 0.010*hrod + 0.009*pls + 0.009*fw + 0.009*going + 0.008*pm\n",
      "INFO : topic #4 (0.050): 0.011*thanks + 0.009*speech + 0.008*mtg + 0.008*night + 0.008*said + 0.007*time + 0.007*pls + 0.006*ok + 0.006*sure + 0.006*want\n",
      "INFO : topic #11 (0.050): 0.021*pls + 0.015*add + 0.014*list + 0.010*mtg + 0.009*ok + 0.009*know + 0.009*want + 0.008*need + 0.008*state + 0.007*let\n",
      "INFO : topic diff=0.465921, rho=0.447214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.64 s\n"
     ]
    }
   ],
   "source": [
    "hillary_mm_corpus = gensim.corpora.MmCorpus('emails_bow.mm')\n",
    "print(hillary_mm_corpus)\n",
    "%time clipped_corpus = gensim.utils.ClippedCorpus(hillary_emails_corpus,1600)\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=20, id2word=vocab, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.050): 0.012*pls + 0.011*told + 0.009*state + 0.009*like + 0.009*lewis + 0.008*address + 0.007*plan + 0.007*work + 0.007*thank + 0.007*list\n",
      "INFO : topic #1 (0.050): 0.012*tomorrow + 0.010*let + 0.008*state + 0.007*thx + 0.006*pls + 0.006*try + 0.006*welcome + 0.006*check + 0.006*fm + 0.006*think\n",
      "INFO : topic #2 (0.050): 0.024*com + 0.023*clintonemail + 0.023*hrod + 0.013*pm + 0.010*state + 0.010*gov + 0.010*diplomacy + 0.009*sunday + 0.008*need + 0.008*sid\n",
      "INFO : topic #3 (0.050): 0.011*fyi + 0.010*let + 0.010*discuss + 0.009*today + 0.008*know + 0.008*tomorrow + 0.008*afghanistan + 0.007*email + 0.007*obama + 0.007*british\n",
      "INFO : topic #4 (0.050): 0.011*thanks + 0.009*speech + 0.008*mtg + 0.008*night + 0.008*said + 0.007*time + 0.007*pls + 0.006*ok + 0.006*sure + 0.006*want\n",
      "INFO : topic #5 (0.050): 0.018*state + 0.015*gov + 0.014*thx + 0.013*send + 0.010*letter + 0.010*hrod + 0.009*pls + 0.009*fw + 0.009*going + 0.008*pm\n",
      "INFO : topic #6 (0.050): 0.042*thx + 0.015*pls + 0.010*think + 0.009*better + 0.008*left + 0.007*home + 0.007*good + 0.007*need + 0.006*discuss + 0.005*work\n",
      "INFO : topic #7 (0.050): 0.013*release + 0.012*tomorrow + 0.011*thx + 0.011*dc + 0.010*asked + 0.010*work + 0.009*good + 0.008*come + 0.008*pls + 0.008*know\n",
      "INFO : topic #8 (0.050): 0.065*ok + 0.062*state + 0.061*gov + 0.038*com + 0.037*clintonemail + 0.036*hrod + 0.018*pm + 0.012*wednesday + 0.011*sullivanjj + 0.011*thx\n",
      "INFO : topic #9 (0.050): 0.011*com + 0.011*know + 0.008*want + 0.008*hrod + 0.007*clintonemail + 0.007*time + 0.007*pis + 0.007*dc + 0.006*let + 0.006*mentioned\n",
      "INFO : topic #10 (0.050): 0.023*know + 0.014*http + 0.013*com + 0.011*time + 0.010*let + 0.010*thx + 0.008*www + 0.008*want + 0.007*home + 0.007*free\n",
      "INFO : topic #11 (0.050): 0.021*pls + 0.015*add + 0.014*list + 0.010*mtg + 0.009*ok + 0.009*know + 0.009*want + 0.008*need + 0.008*state + 0.007*let\n",
      "INFO : topic #12 (0.050): 0.033*com + 0.033*hrod + 0.032*clintonemail + 0.021*pm + 0.017*time + 0.015*gov + 0.014*state + 0.014*schedule + 0.011*saturday + 0.011*thx\n",
      "INFO : topic #13 (0.050): 0.045*state + 0.017*pm + 0.016*department + 0.015*house + 0.014*benghazi + 0.014*subject + 0.011*dept + 0.011*produced + 0.011*waiver + 0.010*information\n",
      "INFO : topic #14 (0.050): 0.080*print + 0.051*pis + 0.039*pls + 0.019*hrod + 0.017*clintonemail + 0.017*fw + 0.016*com + 0.013*pm + 0.011*jilotylc + 0.010*mtg\n",
      "INFO : topic #15 (0.050): 0.029*thx + 0.021*yes + 0.019*tomorrow + 0.018*talk + 0.012*let + 0.011*early + 0.009*today + 0.009*know + 0.008*meet + 0.008*ready\n",
      "INFO : topic #16 (0.050): 0.014*ok + 0.013*forward + 0.012*year + 0.012*best + 0.009*agree + 0.009*thanks + 0.009*new + 0.009*coming + 0.008*know + 0.007*tomorrow\n",
      "INFO : topic #17 (0.050): 0.028*pls + 0.019*print + 0.008*way + 0.007*com + 0.007*right + 0.007*called + 0.007*state + 0.006*hrod + 0.006*pm + 0.006*good\n",
      "INFO : topic #18 (0.050): 0.009*talk + 0.007*libya + 0.007*security + 0.006*government + 0.006*romney + 0.005*ntc + 0.005*national + 0.005*jalil + 0.005*fine + 0.005*let\n",
      "INFO : topic #19 (0.050): 0.014*pls + 0.013*parlak + 0.010*know + 0.008*let + 0.007*immigration + 0.007*set + 0.006*later + 0.006*need + 0.006*judith + 0.005*courts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*pls + 0.011*told + 0.009*state + 0.009*like + 0.009*lewis + '\n",
      "  '0.008*address + 0.007*plan + 0.007*work + 0.007*thank + 0.007*list'),\n",
      " (1,\n",
      "  '0.012*tomorrow + 0.010*let + 0.008*state + 0.007*thx + 0.006*pls + '\n",
      "  '0.006*try + 0.006*welcome + 0.006*check + 0.006*fm + 0.006*think'),\n",
      " (2,\n",
      "  '0.024*com + 0.023*clintonemail + 0.023*hrod + 0.013*pm + 0.010*state + '\n",
      "  '0.010*gov + 0.010*diplomacy + 0.009*sunday + 0.008*need + 0.008*sid'),\n",
      " (3,\n",
      "  '0.011*fyi + 0.010*let + 0.010*discuss + 0.009*today + 0.008*know + '\n",
      "  '0.008*tomorrow + 0.008*afghanistan + 0.007*email + 0.007*obama + '\n",
      "  '0.007*british'),\n",
      " (4,\n",
      "  '0.011*thanks + 0.009*speech + 0.008*mtg + 0.008*night + 0.008*said + '\n",
      "  '0.007*time + 0.007*pls + 0.006*ok + 0.006*sure + 0.006*want'),\n",
      " (5,\n",
      "  '0.018*state + 0.015*gov + 0.014*thx + 0.013*send + 0.010*letter + '\n",
      "  '0.010*hrod + 0.009*pls + 0.009*fw + 0.009*going + 0.008*pm'),\n",
      " (6,\n",
      "  '0.042*thx + 0.015*pls + 0.010*think + 0.009*better + 0.008*left + '\n",
      "  '0.007*home + 0.007*good + 0.007*need + 0.006*discuss + 0.005*work'),\n",
      " (7,\n",
      "  '0.013*release + 0.012*tomorrow + 0.011*thx + 0.011*dc + 0.010*asked + '\n",
      "  '0.010*work + 0.009*good + 0.008*come + 0.008*pls + 0.008*know'),\n",
      " (8,\n",
      "  '0.065*ok + 0.062*state + 0.061*gov + 0.038*com + 0.037*clintonemail + '\n",
      "  '0.036*hrod + 0.018*pm + 0.012*wednesday + 0.011*sullivanjj + 0.011*thx'),\n",
      " (9,\n",
      "  '0.011*com + 0.011*know + 0.008*want + 0.008*hrod + 0.007*clintonemail + '\n",
      "  '0.007*time + 0.007*pis + 0.007*dc + 0.006*let + 0.006*mentioned'),\n",
      " (10,\n",
      "  '0.023*know + 0.014*http + 0.013*com + 0.011*time + 0.010*let + 0.010*thx + '\n",
      "  '0.008*www + 0.008*want + 0.007*home + 0.007*free'),\n",
      " (11,\n",
      "  '0.021*pls + 0.015*add + 0.014*list + 0.010*mtg + 0.009*ok + 0.009*know + '\n",
      "  '0.009*want + 0.008*need + 0.008*state + 0.007*let'),\n",
      " (12,\n",
      "  '0.033*com + 0.033*hrod + 0.032*clintonemail + 0.021*pm + 0.017*time + '\n",
      "  '0.015*gov + 0.014*state + 0.014*schedule + 0.011*saturday + 0.011*thx'),\n",
      " (13,\n",
      "  '0.045*state + 0.017*pm + 0.016*department + 0.015*house + 0.014*benghazi + '\n",
      "  '0.014*subject + 0.011*dept + 0.011*produced + 0.011*waiver + '\n",
      "  '0.010*information'),\n",
      " (14,\n",
      "  '0.080*print + 0.051*pis + 0.039*pls + 0.019*hrod + 0.017*clintonemail + '\n",
      "  '0.017*fw + 0.016*com + 0.013*pm + 0.011*jilotylc + 0.010*mtg'),\n",
      " (15,\n",
      "  '0.029*thx + 0.021*yes + 0.019*tomorrow + 0.018*talk + 0.012*let + '\n",
      "  '0.011*early + 0.009*today + 0.009*know + 0.008*meet + 0.008*ready'),\n",
      " (16,\n",
      "  '0.014*ok + 0.013*forward + 0.012*year + 0.012*best + 0.009*agree + '\n",
      "  '0.009*thanks + 0.009*new + 0.009*coming + 0.008*know + 0.007*tomorrow'),\n",
      " (17,\n",
      "  '0.028*pls + 0.019*print + 0.008*way + 0.007*com + 0.007*right + '\n",
      "  '0.007*called + 0.007*state + 0.006*hrod + 0.006*pm + 0.006*good'),\n",
      " (18,\n",
      "  '0.009*talk + 0.007*libya + 0.007*security + 0.006*government + 0.006*romney '\n",
      "  '+ 0.005*ntc + 0.005*national + 0.005*jalil + 0.005*fine + 0.005*let'),\n",
      " (19,\n",
      "  '0.014*pls + 0.013*parlak + 0.010*know + 0.008*let + 0.007*immigration + '\n",
      "  '0.007*set + 0.006*later + 0.006*need + 0.006*judith + 0.005*courts')]\n",
      "[(14, 0.95869565183136785)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics(-1))\n",
    "pprint(lda_model.get_document_topics(vectors_doc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(hillary_mm_corpus, id2word=vocab)\n",
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[hillary_mm_corpus], id2word=vocab, num_topics=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"And now for something completely different. They refuse to permit us to obtain the refuse permit. The grand jury commented on a number of other topics, among them the Atlanta and Fulton County purchasing departments which it said, ``Are well operated and follow generally accepted practices which inure to the best interest of both governments?''\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokens = sentence.apply(tokenize)\n",
    "rawtokens = word_tokenize(sentence)\n",
    "tokens = [token for token in rawtokens if token not in STOPWORDS]\n",
    "#print(tokens)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "words = [word for (word,tag) in tagged_tokens if tag.startswith('NN')]\n",
    "#print(words)\n",
    "words_arr = []\n",
    "words_arr.append(words)\n",
    "print(words_arr)\n",
    "%time vocab = gensim.corpora.Dictionary(words_arr)\n",
    "vectors = vocab.doc2bow(words_arr[0])\n",
    "print(vectors)\n",
    "corpus = HillaryEmails(vectors)\n",
    "print(corpus)\n",
    "#%time gensim.corpora.MmCorpus.serialize('emails_bow.mm',corpus)\n",
    "clipped = gensim.utils.ClippedCorpus(corpus,1600)\n",
    "print(clipped)\n",
    "#%time lda_model = gensim.models.LdaModel(clipped, num_topics=2, id2word=vocab, passes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running R from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "base = importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "packnames = ['ggplot2']\n",
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install)>0:\n",
    "    utils.install_packages(StrVector(names_to_install))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "n_instances = 100\n",
    "subj_docs = [(sent,'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent,'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "# subj_docs[10],obj_docs[1]\n",
    "train_docs = subj_docs[:80]+obj_docs[:80]\n",
    "test_docs = subj_docs[80:100]+obj_docs[80:100]\n",
    "senti_analyser = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = \"He is a murderer\"\n",
    "sentiAnalyser = SentimentIntensityAnalyzer()\n",
    "sentiAnalyser.polarity_scores(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sentiment of emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSentiment(text):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    neu = 0\n",
    "    if text=='':\n",
    "        return 0,pos,neg\n",
    "    global sentiAnalyser\n",
    "    text = text.replace('\\n',' ')\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [sent for sent in sentences if len(sent.split())>2]\n",
    "    for sent in sentences:\n",
    "        senti = sentiAnalyser.polarity_scores(sent)\n",
    "        if senti['neu']>=0.9:\n",
    "            neu += 1\n",
    "        else:\n",
    "            pos += senti['pos']\n",
    "            neg += senti['neg']\n",
    "    count = len(sentences)-neu\n",
    "    if count == 0:\n",
    "        return len(sentences),pos,neg\n",
    "    return len(sentences),pos/count,neg/count\n",
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "sql = \"\"\"select e.Id, p.Name Sender, \n",
    "e.SenderPersonId Sender_Id, e.ExtractedDateSent, e.ExtractedBodyText\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "emails['ExtractedDateSent']=emails.ExtractedDateSent.apply(clean_date)\n",
    "# Step 1: Mails from Hillary\n",
    "emails = emails[emails['Sender_Id']==80]\n",
    "# emails = emails.head(5)\n",
    "# emails.ExtractedBodyText.apply(getSentiment)\n",
    "print('Total Email count by Hillary: ',emails.Id.count())\n",
    "sentiAnalyser = SentimentIntensityAnalyzer()\n",
    "%time sentiments = emails.ExtractedBodyText.apply(getSentiment)\n",
    "senti_map = zip(emails.Id.values,sentiments.values)\n",
    "senti_map = sorted(senti_map,key = lambda item:item[1][2],reverse=True)\n",
    "print(senti_map[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find occurance of country name in mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_count(text):\n",
    "    global country_counter\n",
    "    global countries_new_name\n",
    "    for indx,country in enumerate(pycountry.countries):\n",
    "        name = country.name\n",
    "        if name in countries_new_name.keys():\n",
    "            name = countries_new_name[name]\n",
    "        name = '\\\\b'+name # Ensuring substring start with country name. eg.Indian, Syrian, India\n",
    "        all_matches = re.finditer(name,text,re.IGNORECASE)\n",
    "        n_occurrences = len(list(all_matches))\n",
    "        country_counter[indx] += n_occurrences\n",
    "country_counter = [0 for country in pycountry.countries]\n",
    "country_names = [country.name for country in pycountry.countries]\n",
    "countries_new_name = {'Syrian Arab Republic':'Syria','Russian Federation':'Russia','Iran, Islamic Republic of':\\\n",
    "                      'Iran','Korea, Democratic People\\'s Republic of':'Korea','Palestine, State of':'Palestine',\\\n",
    "                      'Venezuela, Bolivarian Republic of':'Venezuela'}\n",
    "emails.ExtractedBodyText.apply(get_count)\n",
    "# let's study count for some countries\n",
    "country2ind = {}\n",
    "for indx,country in enumerate(pycountry.countries):\n",
    "    country2ind[country.name]=indx\n",
    "country_name_count = zip(country_names,country_counter)\n",
    "country_name_count = sorted(country_name_count,key = lambda(item):item[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = pd.DataFrame(np.array(country_counter),country_names)\n",
    "df = df[df[0]>1]\n",
    "ax = df.plot(kind='bar',legend=False,title='Occurance of Country names')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between country vs average sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_occurence_ct(text,country):\n",
    "    name = country.name\n",
    "    counter = 0\n",
    "    global countries_new_name\n",
    "    if name in countries_new_name.keys():\n",
    "        name = countries_new_name[name]\n",
    "    name = '\\\\b'+name # Ensuring substring start with country name. eg.Indian, Syrian, India\n",
    "    all_matches = re.finditer(name,text,re.IGNORECASE)\n",
    "    n_occurrences = len(list(all_matches))\n",
    "    counter += n_occurrences\n",
    "    abbr_to_ignore = [\"RE\", \"FM\", \"TV\", \"AL\", \"AQ\", \"LA\", \"BEN\"]\n",
    "    if country.alpha2 not in abbr_to_ignore:\n",
    "        name = '\\\\b'+country.alpha2+'\\\\b'\n",
    "        all_matches = re.finditer(name,text)\n",
    "        n_occurrences = len(list(all_matches))\n",
    "        counter += n_occurrences\n",
    "    if country.alpha3 not in abbr_to_ignore:\n",
    "        name = '\\\\b'+country.alpha3+'\\\\b'\n",
    "        all_matches = re.finditer(name,text)\n",
    "        n_occurrences = len(list(all_matches))\n",
    "        counter += n_occurrences\n",
    "    return counter\n",
    "def get_country_sentiment(country):\n",
    "    global sentiments\n",
    "    global emails\n",
    "    res_pos,res_neg = 0,0\n",
    "    counts = emails.ExtractedBodyText.apply(get_occurence_ct,args=(country,))\n",
    "    sentiments_pos = [tpl[1] for tpl in sentiments.values]\n",
    "    sentiments_neg = [tpl[2] for tpl in sentiments.values]\n",
    "    if sum(counts.values)>0:\n",
    "        res_pos = sum([ct*pos for ct,pos in zip(counts.values,sentiments_pos)])/sum(counts.values)\n",
    "        res_neg = sum([ct*neg for ct,neg in zip(counts.values,sentiments_neg)])/sum(counts.values)\n",
    "    return res_pos,res_neg\n",
    "countries = pd.Series([country for country in pycountry.countries])\n",
    "senti = countries.apply(get_country_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_pos = np.array([i[0] for i in senti])\n",
    "df = pd.DataFrame(senti_pos,country_names)\n",
    "df = df[df[0]>0.1]\n",
    "ax = df.plot(kind='bar',legend=False,title='Hillary\\'s Sentiment about countries')\n",
    "ax.set_ylabel('postivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_neg = np.array([i[1] for i in senti])\n",
    "df = pd.DataFrame(senti_neg,country_names)\n",
    "df = df[df[0]>0]\n",
    "ax = df.plot(kind='bar',legend=False,title='Hillary\\'s Sentiment about countries')\n",
    "ax.set_ylabel('negativity')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
