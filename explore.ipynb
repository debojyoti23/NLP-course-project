{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "# cur = conn.cursor()\n",
    "# cur.execute('pragma table_info(Emails)')\n",
    "# f_1 = 'Id'\n",
    "# f_2 = 'MetadataTo'\n",
    "# f_3 = 'MetadataFrom'\n",
    "# f_4 = 'ExtractedSubject'\n",
    "# f_5 = 'ExtractedDateSent'\n",
    "# f_6 = 'ExtractedBodyText'\n",
    "# f_7 = 'RawText'\n",
    "# cur.execute('select {c1},{c2},{c3},{c4},{c5},{c6},{c7} from Emails'.\\\n",
    "#             format(c1=f_1,c2=f_2,c3=f_3,c4=f_4,c5=f_5,c6=f_6,c7=f_7))\n",
    "# cur.fetchall()\n",
    "# cur.fetchone()\n",
    "# columns_less = ['id','to','from','sub','date','body','raw']\n",
    "# columns = [str(tpl[1]) for tpl in columns]\n",
    "def clean_subject(sub):\n",
    "    email_type = None\n",
    "    if re.match('^[Ff][Ww]',sub)!=None:\n",
    "        email_type='fw'\n",
    "        sub = re.sub('^[Ff][Ww].*:\\s*(.+)',r'\\1',sub)\n",
    "    elif re.match('^[Rr][Ee]:',sub)!=None:\n",
    "        email_type='re'\n",
    "        sub = re.sub('^[Rr][Ee]:\\s*(.+)',r'\\1',sub)\n",
    "    elif re.match('^[Ff][Vv][Vv]:',sub)!=None:\n",
    "        email_type='fvv'\n",
    "        sub = re.sub('^[Ff][Vv][Vv]:\\s*(.+)',r'\\1',sub)\n",
    "    return email_type,sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "sql = \"\"\"select e.Id, p.Name Sender, e.SenderPersonId Sender_Id\n",
    ", e.MetadataTo Receiver, a.PersonId Receiver_Id, e.ExtractedDateSent, e.ExtractedSubject\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id \n",
    "left outer join Aliases a on lower(e.MetadataTo)=a.Alias\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "temp = emails.ExtractedSubject.apply(clean_subject)\n",
    "emails['ExtractedSubject'] = [tpl[1] for tpl in temp]\n",
    "emails['email_type'] = email_type = [tpl[0] for tpl in temp]\n",
    "emails.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up sender receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch data in pandas dataframe\n",
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "# sql = \"select * from Emails\"\n",
    "# emails = pd.read_sql_query(sql,conn)\n",
    "# columns = ['Id','SenderPersonId','MetadataFrom','MetadataTo','ExtractedDateSent','ExtractedSubject','ExtractedBodyText']\n",
    "# emails[columns].head(5)\n",
    "# emails[emails['SenderPersonId']==80][columns].head(5)\n",
    "# x = emails.groupby('SenderPersonId').size()\n",
    "# print 'No of mails sent by HC =',x[80]\n",
    "\n",
    "# Assign receiver Id\n",
    "sql = \"\"\"select e.Id, p.Name Sender, \n",
    "e.SenderPersonId Sender_Id, e.MetadataTo Receiver, a.PersonId Receiver_Id, e.ExtractedDateSent\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id \n",
    "left outer join Aliases a on lower(e.MetadataTo)=a.Alias\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "# emails_noNA = emails.dropna(how='any')\n",
    "# emails.head(20)\n",
    "idx_NaN = [i for i,val in enumerate(emails['Receiver_Id']) if np.isnan(val)]\n",
    "\n",
    "\n",
    "# Create dictionary personId:name\n",
    "sql = \"select * from Persons\"\n",
    "persons = pd.read_sql_query(sql,conn)\n",
    "persons_dict = {}\n",
    "for entry in persons.itertuples():\n",
    "    persons_dict[entry[2]]=entry[1]\n",
    "    \n",
    "# Fill up missing Receiver_Id fields\n",
    "receiver_dict = {}\n",
    "pattern = re.compile('^(\\w+),\\sp*(\\w+)')\n",
    "for idx in idx_NaN:\n",
    "    name = emails['Receiver'][idx]\n",
    "    pat_obj = pattern.match(name)\n",
    "    if pat_obj!=None:\n",
    "        person_name = pat_obj.group(2)+' '+pat_obj.group(1)\n",
    "        try:\n",
    "            person_id = persons_dict[person_name]\n",
    "            emails['Receiver_Id'][idx]=person_id\n",
    "        except:\n",
    "            pass\n",
    "temp = emails.shape[0]\n",
    "emails = emails.dropna(how='any')\n",
    "print('Discarded emails: ',temp-emails.shape[0])\n",
    "\n",
    "emails.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# txt = 'Thursday, September 30, 2010 9:04 PM'\n",
    "# txt = 'Thu Sep 17 06:03:43 2009'\n",
    "def clean_date_aux(ts):\n",
    "    pat = re.compile('\\'?(?P<weekday>\\w+),?\\s*(?P<month>\\w+) (?P<date>\\d+),?\\s*(?P<year>\\d+) (?P<time>.+) (?P<ampm>[A|P]M)')\n",
    "    pat1 = re.compile('(?P<weekday>.+) (?P<month>.+) (?P<date>.+) (?P<time>.+) (?P<year>.+)')\n",
    "    if ts=='':\n",
    "        return np.NAN\n",
    "    re_obj = None\n",
    "    try:\n",
    "        if pat.match(ts)!=None:\n",
    "            re_obj = pat.match(ts)\n",
    "        elif pat1.match(ts)!=None:\n",
    "            re_obj = pat1.match(ts)\n",
    "        else:\n",
    "            raise\n",
    "        # Process date\n",
    "        month = re_obj.group('month')\n",
    "        day = re_obj.group('date')\n",
    "        year = re_obj.group('year')\n",
    "        time = re_obj.group('time').replace('.',':')\n",
    "        # Check for numeric value of day and year\n",
    "        try:\n",
    "            month = month[:3]\n",
    "            day = int(day)\n",
    "            year = int(year)\n",
    "        except:\n",
    "            raise\n",
    "        # Check time string\n",
    "        if ':' not in time and len(time)>2:\n",
    "            time = time[:-2]+':'+time[-2:]\n",
    "        try:\n",
    "            time = time + ' ' + re_obj.group('ampm')\n",
    "        except:\n",
    "            pass\n",
    "        ts_str = \"{} {} {} {}\".format(month,day,year,time)\n",
    "        try:\n",
    "            pd_ts = pd.to_datetime(ts_str)\n",
    "            return pd_ts\n",
    "        except:\n",
    "            raise\n",
    "    except:\n",
    "        return np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6306"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_date(raw_date):\n",
    "    return clean_date_aux(raw_date)\n",
    "emails['Date']=emails.ExtractedDateSent.apply(clean_date)\n",
    "emails.head(10)\n",
    "# emails = emails.dropna(how='any')\n",
    "emails.index = emails.Date\n",
    "emails.sort_index(inplace=True)\n",
    "mindate,maxdate = emails.index.min(),emails.index.max()\n",
    "\"Date Range from {} to {}\".format(mindate.date(),maxdate.date())\n",
    "emails[:'2011-01'].Id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process email body: Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HillaryEmails(object):\n",
    "    def __init__(self,doc_vectors):\n",
    "        self.doc_vectors = doc_vectors\n",
    "    def __iter__(self):\n",
    "        for vector in self.doc_vectors:\n",
    "            yield vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(4249 unique tokens: ['intervention', 'thomases', 'denigration', 'board', 'ibrahim']...) from 1993 documents (total 17317 corpus positions)\n",
      "INFO : storing corpus in Matrix Market format to emails_bow.mm\n",
      "INFO : saving sparse matrix to emails_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : saved 1993x4249 matrix, density=0.187% (15844/8468257)\n",
      "INFO : saving MmCorpus index to emails_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993\n",
      "Wall time: 31 ms\n",
      "Wall time: 44 ms\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "def do_BoW(tokens):\n",
    "    return vocab.doc2bow(tokens)\n",
    "def get_maxwords(vector):\n",
    "    minimum = int(len(vector)*0.2)\n",
    "    minimum = min(10,minimum)\n",
    "    global vocab\n",
    "    temp = sorted(vector,key=lambda x:x[1],reverse=True)\n",
    "    topic = [vocab[it[0]] for it in temp[:minimum]]\n",
    "    return topic\n",
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "sql = \"\"\"select e.Id, p.Name Sender, \n",
    "e.SenderPersonId Sender_Id, e.ExtractedDateSent, e.ExtractedBodyText\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "#print(emails)\n",
    "emails['ExtractedDateSent']=emails.ExtractedDateSent.apply(clean_date)\n",
    "#print(emails['ExtractedDateSent'])\n",
    "# Step 1: Mails from Hillary\n",
    "emails = emails[emails['Sender_Id']==80]\n",
    "#print(emails)\n",
    "# Step 2: Tokenize email body\n",
    "email_tokens = emails.ExtractedBodyText.apply(tokenize)\n",
    "#print(email_tokens)\n",
    "doc_stream = [tokens for tokens in email_tokens]\n",
    "#print(doc_stream)\n",
    "print(len(doc_stream))\n",
    "# Step 3: Build Vocabulary\n",
    "%time vocab = gensim.corpora.Dictionary(doc_stream)\n",
    "#print(vocab)\n",
    "    # print vocab.items()\n",
    "    # vocab.filter_extremes(no_below=20,no_above=0.8)\n",
    "    # vocab.filter_extremes(no_above=0.5)\n",
    "# Step 4: Run BoW on docs\n",
    "vectors_doc = email_tokens.apply(do_BoW)\n",
    "#print(vectors_doc)\n",
    "maxwords = vectors_doc.apply(get_maxwords)\n",
    "#print(\"max\\n\",maxwords)\n",
    "#Step 5: save doc vectors in mm file\n",
    "hillary_emails_corpus = HillaryEmails(vectors_doc)\n",
    "#print(hillary_emails_corpus)\n",
    "%time gensim.corpora.MmCorpus.serialize('emails_bow.mm',hillary_emails_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from emails_bow.mm.index\n",
      "INFO : initializing corpus reader from emails_bow.mm\n",
      "INFO : accepted corpus with 1993 documents, 4249 features, 15844 non-zero entries\n",
      "INFO : using symmetric alpha at 0.05\n",
      "INFO : using symmetric eta at 0.05\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(1993 documents, 4249 features, 15844 non-zero entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : input corpus stream has no len(); counting documents\n",
      "INFO : running online LDA training, 20 topics, 4 passes over the supplied corpus of 1600 documents, updating model once every 1600 documents, evaluating perplexity every 1600 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : -20.091 per-word bound, 1116939.7 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 0, at document #1600/1600\n",
      "INFO : topic #19 (0.050): 0.008*ll + 0.007*talk + 0.006*email + 0.005*supposed + 0.005*mtg + 0.005*yes + 0.005*state + 0.005*like + 0.004*good + 0.004*want\n",
      "INFO : topic #11 (0.050): 0.015*print + 0.013*pis + 0.008*asked + 0.008*tomorrow + 0.007*com + 0.007*clintonemail + 0.007*good + 0.006*pls + 0.006*hrod + 0.005*thanks\n",
      "INFO : topic #7 (0.050): 0.017*know + 0.016*ok + 0.014*print + 0.013*state + 0.012*talk + 0.012*pis + 0.012*pls + 0.012*hrod + 0.012*com + 0.012*pm\n",
      "INFO : topic #15 (0.050): 0.032*state + 0.026*hrod + 0.026*gov + 0.026*com + 0.025*clintonemail + 0.018*pm + 0.012*fw + 0.012*print + 0.011*thx + 0.011*pls\n",
      "INFO : topic #9 (0.050): 0.019*com + 0.017*hrod + 0.016*clintonemail + 0.012*thx + 0.012*pm + 0.011*state + 0.010*gov + 0.008*let + 0.008*libya + 0.007*hillary\n",
      "INFO : topic diff=13.711454, rho=1.000000\n",
      "INFO : -10.291 per-word bound, 1252.7 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 1, at document #1600/1600\n",
      "INFO : topic #12 (0.050): 0.016*know + 0.015*need + 0.014*time + 0.013*email + 0.011*let + 0.010*work + 0.010*pls + 0.010*lona + 0.008*judith + 0.008*follow\n",
      "INFO : topic #6 (0.050): 0.014*state + 0.014*pis + 0.013*hrod + 0.013*com + 0.012*pls + 0.011*clintonemail + 0.010*thx + 0.009*pm + 0.008*gov + 0.008*print\n",
      "INFO : topic #5 (0.050): 0.013*state + 0.013*know + 0.013*thx + 0.012*let + 0.010*gov + 0.010*yes + 0.007*com + 0.007*time + 0.007*tomorrow + 0.006*want\n",
      "INFO : topic #0 (0.050): 0.030*ok + 0.018*know + 0.013*thx + 0.012*want + 0.008*calling + 0.007*need + 0.007*send + 0.007*hope + 0.007*thank + 0.007*time\n",
      "INFO : topic #17 (0.050): 0.015*need + 0.012*com + 0.009*hrod + 0.009*clintonemail + 0.008*state + 0.007*gov + 0.006*pls + 0.006*statement + 0.006*like + 0.006*ny\n",
      "INFO : topic diff=1.064070, rho=0.577350\n",
      "INFO : -9.735 per-word bound, 852.4 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 2, at document #1600/1600\n",
      "INFO : topic #8 (0.050): 0.050*state + 0.020*gov + 0.011*house + 0.011*cheryl + 0.010*dept + 0.010*benghazi + 0.010*subject + 0.009*department + 0.009*sensitive + 0.009*redactions\n",
      "INFO : topic #17 (0.050): 0.018*need + 0.009*com + 0.009*going + 0.007*statement + 0.007*hrod + 0.007*clintonemail + 0.007*ny + 0.006*like + 0.006*home + 0.006*talk\n",
      "INFO : topic #18 (0.050): 0.085*print + 0.066*pls + 0.025*pis + 0.015*com + 0.015*hrod + 0.014*clintonemail + 0.008*sorry + 0.007*state + 0.006*sunday + 0.006*congrats\n",
      "INFO : topic #6 (0.050): 0.016*pis + 0.012*pls + 0.012*state + 0.011*hrod + 0.011*thx + 0.010*com + 0.010*clintonemail + 0.008*tell + 0.008*year + 0.008*fax\n",
      "INFO : topic #16 (0.050): 0.035*thx + 0.013*com + 0.012*http + 0.008*www + 0.008*parlak + 0.006*pis + 0.006*discuss + 0.005*week + 0.005*million + 0.005*send\n",
      "INFO : topic diff=0.712863, rho=0.500000\n",
      "INFO : -9.531 per-word bound, 739.7 perplexity estimate based on a held-out corpus of 1600 documents with 14746 words\n",
      "INFO : PROGRESS: pass 3, at document #1600/1600\n",
      "INFO : topic #18 (0.050): 0.100*print + 0.075*pls + 0.032*pis + 0.012*com + 0.012*hrod + 0.012*clintonemail + 0.008*sorry + 0.007*congrats + 0.006*good + 0.005*sunday\n",
      "INFO : topic #4 (0.050): 0.020*tomorrow + 0.020*pls + 0.015*thx + 0.014*talk + 0.013*time + 0.011*let + 0.011*like + 0.011*want + 0.010*schedule + 0.009*monday\n",
      "INFO : topic #13 (0.050): 0.132*ok + 0.017*thx + 0.013*pls + 0.010*let + 0.009*try + 0.008*hope + 0.008*pis + 0.007*send + 0.006*release + 0.006*left\n",
      "INFO : topic #11 (0.050): 0.012*asked + 0.009*speech + 0.008*pis + 0.007*good + 0.007*great + 0.007*reason + 0.006*jones + 0.006*print + 0.006*thanks + 0.006*tomorrow\n",
      "INFO : topic #8 (0.050): 0.052*state + 0.019*gov + 0.013*house + 0.011*dept + 0.011*subject + 0.011*benghazi + 0.011*cheryl + 0.011*department + 0.010*sensitive + 0.010*redactions\n",
      "INFO : topic diff=0.475421, rho=0.447214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "hillary_mm_corpus = gensim.corpora.MmCorpus('emails_bow.mm')\n",
    "print(hillary_mm_corpus)\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(hillary_emails_corpus,1600)\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=20, id2word=vocab, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.050): 0.019*ok + 0.015*know + 0.013*want + 0.011*calling + 0.009*thank + 0.009*thx + 0.009*send + 0.008*hope + 0.008*need + 0.007*btw\n",
      "INFO : topic #1 (0.050): 0.019*fyi + 0.018*today + 0.015*want + 0.014*gov + 0.013*good + 0.011*state + 0.010*com + 0.010*tomorrow + 0.010*clintonemail + 0.008*come\n",
      "INFO : topic #2 (0.050): 0.017*pm + 0.016*state + 0.011*like + 0.009*office + 0.009*department + 0.007*ok + 0.007*time + 0.007*think + 0.006*house + 0.006*romney\n",
      "INFO : topic #3 (0.050): 0.027*pls + 0.016*set + 0.015*today + 0.010*diplomacy + 0.009*thought + 0.008*list + 0.008*heard + 0.008*team + 0.007*add + 0.007*told\n",
      "INFO : topic #4 (0.050): 0.020*tomorrow + 0.020*pls + 0.015*thx + 0.014*talk + 0.013*time + 0.011*let + 0.011*like + 0.011*want + 0.010*schedule + 0.009*monday\n",
      "INFO : topic #5 (0.050): 0.017*yes + 0.014*thx + 0.013*know + 0.013*let + 0.009*state + 0.007*said + 0.007*time + 0.007*want + 0.007*gov + 0.006*tomorrow\n",
      "INFO : topic #6 (0.050): 0.017*pis + 0.012*pls + 0.011*thx + 0.010*state + 0.009*hrod + 0.009*tell + 0.009*fax + 0.008*year + 0.008*com + 0.008*email\n",
      "INFO : topic #7 (0.050): 0.037*know + 0.032*talk + 0.019*let + 0.013*free + 0.012*ok + 0.010*berry + 0.009*week + 0.009*pm + 0.009*pls + 0.008*hrod\n",
      "INFO : topic #8 (0.050): 0.052*state + 0.019*gov + 0.013*house + 0.011*dept + 0.011*subject + 0.011*benghazi + 0.011*cheryl + 0.011*department + 0.010*sensitive + 0.010*redactions\n",
      "INFO : topic #9 (0.050): 0.013*com + 0.012*hrod + 0.011*clintonemail + 0.009*pm + 0.008*thx + 0.008*state + 0.008*gov + 0.007*libya + 0.006*hillary + 0.006*let\n",
      "INFO : topic #10 (0.050): 0.020*agree + 0.013*thx + 0.010*let + 0.007*work + 0.007*want + 0.007*help + 0.007*think + 0.007*asap + 0.007*draft + 0.006*talk\n",
      "INFO : topic #11 (0.050): 0.012*asked + 0.009*speech + 0.008*pis + 0.007*good + 0.007*great + 0.007*reason + 0.006*jones + 0.006*print + 0.006*thanks + 0.006*tomorrow\n",
      "INFO : topic #12 (0.050): 0.020*time + 0.020*know + 0.017*need + 0.015*email + 0.013*lona + 0.012*let + 0.011*work + 0.011*hope + 0.010*pls + 0.010*copying\n",
      "INFO : topic #13 (0.050): 0.132*ok + 0.017*thx + 0.013*pls + 0.010*let + 0.009*try + 0.008*hope + 0.008*pis + 0.007*send + 0.006*release + 0.006*left\n",
      "INFO : topic #14 (0.050): 0.014*work + 0.014*release + 0.012*discuss + 0.012*let + 0.011*jake + 0.010*thanks + 0.009*week + 0.008*tell + 0.008*pls + 0.008*think\n",
      "INFO : topic #15 (0.050): 0.046*hrod + 0.045*state + 0.045*com + 0.044*clintonemail + 0.040*gov + 0.026*pm + 0.016*fw + 0.010*print + 0.009*saturday + 0.008*sunday\n",
      "INFO : topic #16 (0.050): 0.033*thx + 0.013*com + 0.013*http + 0.008*www + 0.008*parlak + 0.006*pis + 0.005*million + 0.005*html + 0.005*week + 0.005*discuss\n",
      "INFO : topic #17 (0.050): 0.019*need + 0.010*going + 0.008*statement + 0.007*com + 0.007*home + 0.007*ny + 0.007*like + 0.006*talk + 0.006*send + 0.006*clintonemail\n",
      "INFO : topic #18 (0.050): 0.100*print + 0.075*pls + 0.032*pis + 0.012*com + 0.012*hrod + 0.012*clintonemail + 0.008*sorry + 0.007*congrats + 0.006*good + 0.005*sunday\n",
      "INFO : topic #19 (0.050): 0.010*mtg + 0.009*ll + 0.009*supposed + 0.007*email + 0.006*address + 0.006*potus + 0.006*add + 0.005*talk + 0.005*good + 0.005*start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*ok + 0.015*know + 0.013*want + 0.011*calling + 0.009*thank + 0.009*thx + 0.009*send + 0.008*hope + 0.008*need + 0.007*btw'),\n",
       " (1,\n",
       "  '0.019*fyi + 0.018*today + 0.015*want + 0.014*gov + 0.013*good + 0.011*state + 0.010*com + 0.010*tomorrow + 0.010*clintonemail + 0.008*come'),\n",
       " (2,\n",
       "  '0.017*pm + 0.016*state + 0.011*like + 0.009*office + 0.009*department + 0.007*ok + 0.007*time + 0.007*think + 0.006*house + 0.006*romney'),\n",
       " (3,\n",
       "  '0.027*pls + 0.016*set + 0.015*today + 0.010*diplomacy + 0.009*thought + 0.008*list + 0.008*heard + 0.008*team + 0.007*add + 0.007*told'),\n",
       " (4,\n",
       "  '0.020*tomorrow + 0.020*pls + 0.015*thx + 0.014*talk + 0.013*time + 0.011*let + 0.011*like + 0.011*want + 0.010*schedule + 0.009*monday'),\n",
       " (5,\n",
       "  '0.017*yes + 0.014*thx + 0.013*know + 0.013*let + 0.009*state + 0.007*said + 0.007*time + 0.007*want + 0.007*gov + 0.006*tomorrow'),\n",
       " (6,\n",
       "  '0.017*pis + 0.012*pls + 0.011*thx + 0.010*state + 0.009*hrod + 0.009*tell + 0.009*fax + 0.008*year + 0.008*com + 0.008*email'),\n",
       " (7,\n",
       "  '0.037*know + 0.032*talk + 0.019*let + 0.013*free + 0.012*ok + 0.010*berry + 0.009*week + 0.009*pm + 0.009*pls + 0.008*hrod'),\n",
       " (8,\n",
       "  '0.052*state + 0.019*gov + 0.013*house + 0.011*dept + 0.011*subject + 0.011*benghazi + 0.011*cheryl + 0.011*department + 0.010*sensitive + 0.010*redactions'),\n",
       " (9,\n",
       "  '0.013*com + 0.012*hrod + 0.011*clintonemail + 0.009*pm + 0.008*thx + 0.008*state + 0.008*gov + 0.007*libya + 0.006*hillary + 0.006*let'),\n",
       " (10,\n",
       "  '0.020*agree + 0.013*thx + 0.010*let + 0.007*work + 0.007*want + 0.007*help + 0.007*think + 0.007*asap + 0.007*draft + 0.006*talk'),\n",
       " (11,\n",
       "  '0.012*asked + 0.009*speech + 0.008*pis + 0.007*good + 0.007*great + 0.007*reason + 0.006*jones + 0.006*print + 0.006*thanks + 0.006*tomorrow'),\n",
       " (12,\n",
       "  '0.020*time + 0.020*know + 0.017*need + 0.015*email + 0.013*lona + 0.012*let + 0.011*work + 0.011*hope + 0.010*pls + 0.010*copying'),\n",
       " (13,\n",
       "  '0.132*ok + 0.017*thx + 0.013*pls + 0.010*let + 0.009*try + 0.008*hope + 0.008*pis + 0.007*send + 0.006*release + 0.006*left'),\n",
       " (14,\n",
       "  '0.014*work + 0.014*release + 0.012*discuss + 0.012*let + 0.011*jake + 0.010*thanks + 0.009*week + 0.008*tell + 0.008*pls + 0.008*think'),\n",
       " (15,\n",
       "  '0.046*hrod + 0.045*state + 0.045*com + 0.044*clintonemail + 0.040*gov + 0.026*pm + 0.016*fw + 0.010*print + 0.009*saturday + 0.008*sunday'),\n",
       " (16,\n",
       "  '0.033*thx + 0.013*com + 0.013*http + 0.008*www + 0.008*parlak + 0.006*pis + 0.005*million + 0.005*html + 0.005*week + 0.005*discuss'),\n",
       " (17,\n",
       "  '0.019*need + 0.010*going + 0.008*statement + 0.007*com + 0.007*home + 0.007*ny + 0.007*like + 0.006*talk + 0.006*send + 0.006*clintonemail'),\n",
       " (18,\n",
       "  '0.100*print + 0.075*pls + 0.032*pis + 0.012*com + 0.012*hrod + 0.012*clintonemail + 0.008*sorry + 0.007*congrats + 0.006*good + 0.005*sunday'),\n",
       " (19,\n",
       "  '0.010*mtg + 0.009*ll + 0.009*supposed + 0.007*email + 0.006*address + 0.006*potus + 0.006*add + 0.005*talk + 0.005*good + 0.005*start')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : calculating IDF weights for 1993 documents and 4248 features (15844 matrix non-zeros)\n",
      "INFO : using serial LSI version on this node\n",
      "INFO : updating model with new documents\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (4249, 300) action matrix\n",
      "INFO : orthonormalizing (4249, 300) action matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 2nd phase: running dense svd on (300, 1993) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 12.506% of energy spectrum)\n",
      "INFO : processed documents up to #1993\n",
      "INFO : topic #0(9.460): 0.995*\"ok\" + 0.074*\"thx\" + 0.033*\"pls\" + 0.024*\"print\" + 0.017*\"talk\" + 0.010*\"dc\" + 0.009*\"pis\" + 0.008*\"let\" + 0.008*\"clintonemail\" + 0.008*\"tomorrow\"\n",
      "INFO : topic #1(8.125): 0.787*\"print\" + 0.530*\"pls\" + 0.268*\"pis\" + 0.049*\"gov\" + 0.047*\"state\" + -0.045*\"ok\" + 0.044*\"hrod\" + 0.044*\"clintonemail\" + 0.043*\"copies\" + 0.043*\"com\"\n",
      "INFO : topic #2(5.791): 0.869*\"thx\" + 0.144*\"state\" + 0.143*\"gov\" + 0.139*\"hrod\" + 0.139*\"clintonemail\" + 0.138*\"com\" + 0.109*\"pm\" + 0.098*\"yes\" + -0.096*\"print\" + 0.085*\"talk\"\n",
      "INFO : topic #3(5.447): -0.429*\"thx\" + 0.337*\"gov\" + 0.335*\"state\" + 0.309*\"hrod\" + 0.308*\"clintonemail\" + 0.307*\"com\" + 0.235*\"pm\" + 0.174*\"fw\" + 0.136*\"jilotylc\" + -0.113*\"pls\"\n",
      "INFO : topic #4(4.831): -0.807*\"pis\" + 0.551*\"pls\" + -0.120*\"print\" + 0.045*\"state\" + 0.045*\"talk\" + 0.043*\"gov\" + 0.041*\"clintonemail\" + 0.039*\"hrod\" + 0.039*\"com\" + 0.037*\"release\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 383 ms\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(hillary_mm_corpus, id2word=vocab)\n",
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[hillary_mm_corpus], id2word=vocab, num_topics=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running R from python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "base = importr('base')\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''packnames = ['ggplot2']\n",
    "names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n",
    "if len(names_to_install)>0:\n",
    "    utils.install_packages(StrVector(names_to_install))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "n_instances = 100\n",
    "subj_docs = [(sent,'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent,'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "# subj_docs[10],obj_docs[1]\n",
    "train_docs = subj_docs[:80]+obj_docs[:80]\n",
    "test_docs = subj_docs[80:100]+obj_docs[80:100]\n",
    "senti_analyser = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': -0.6808, 'neg': 0.697, 'neu': 0.303, 'pos': 0.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"He is a murderer\"\n",
    "sentiAnalyser = SentimentIntensityAnalyzer()\n",
    "sentiAnalyser.polarity_scores(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sentiment of emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Email count by Hillary:  1993\n",
      "Wall time: 743 ms\n",
      "[(552, (7, 0.0, 0.772)), (2240, (1, 0.0, 0.7)), (585, (3, 0.0, 0.531)), (1022, (2, 0.0, 0.508)), (2308, (1, 0.0, 0.483))]\n"
     ]
    }
   ],
   "source": [
    "def getSentiment(text):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    neu = 0\n",
    "    if text=='':\n",
    "        return 0,pos,neg\n",
    "    global sentiAnalyser\n",
    "    text = text.replace('\\n',' ')\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [sent for sent in sentences if len(sent.split())>2]\n",
    "    for sent in sentences:\n",
    "        senti = sentiAnalyser.polarity_scores(sent)\n",
    "        if senti['neu']>=0.9:\n",
    "            neu += 1\n",
    "        else:\n",
    "            pos += senti['pos']\n",
    "            neg += senti['neg']\n",
    "    count = len(sentences)-neu\n",
    "    if count == 0:\n",
    "        return len(sentences),pos,neg\n",
    "    return len(sentences),pos/count,neg/count\n",
    "conn = sqlite3.connect('../hillary-clinton-emails/database.sqlite')\n",
    "sql = \"\"\"select e.Id, p.Name Sender, \n",
    "e.SenderPersonId Sender_Id, e.ExtractedDateSent, e.ExtractedBodyText\n",
    "from Emails e \n",
    "inner join Persons p on e.SenderPersonId=p.Id\n",
    "\"\"\"\n",
    "emails = pd.read_sql_query(sql,conn)\n",
    "emails['ExtractedDateSent']=emails.ExtractedDateSent.apply(clean_date)\n",
    "# Step 1: Mails from Hillary\n",
    "emails = emails[emails['Sender_Id']==80]\n",
    "# emails = emails.head(5)\n",
    "# emails.ExtractedBodyText.apply(getSentiment)\n",
    "print('Total Email count by Hillary: ',emails.Id.count())\n",
    "sentiAnalyser = SentimentIntensityAnalyzer()\n",
    "%time sentiments = emails.ExtractedBodyText.apply(getSentiment)\n",
    "senti_map = zip(emails.Id.values,sentiments.values)\n",
    "senti_map = sorted(senti_map,key = lambda item:item[1][2],reverse=True)\n",
    "print(senti_map[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find occurance of country name in mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pycountry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a75e0d450fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpycountry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pycountry'"
     ]
    }
   ],
   "source": [
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_count(text):\n",
    "    global country_counter\n",
    "    global countries_new_name\n",
    "    for indx,country in enumerate(pycountry.countries):\n",
    "        name = country.name\n",
    "        if name in countries_new_name.keys():\n",
    "            name = countries_new_name[name]\n",
    "        name = '\\\\b'+name # Ensuring substring start with country name. eg.Indian, Syrian, India\n",
    "        all_matches = re.finditer(name,text,re.IGNORECASE)\n",
    "        n_occurrences = len(list(all_matches))\n",
    "        country_counter[indx] += n_occurrences\n",
    "country_counter = [0 for country in pycountry.countries]\n",
    "country_names = [country.name for country in pycountry.countries]\n",
    "countries_new_name = {'Syrian Arab Republic':'Syria','Russian Federation':'Russia','Iran, Islamic Republic of':\\\n",
    "                      'Iran','Korea, Democratic People\\'s Republic of':'Korea','Palestine, State of':'Palestine',\\\n",
    "                      'Venezuela, Bolivarian Republic of':'Venezuela'}\n",
    "emails.ExtractedBodyText.apply(get_count)\n",
    "# let's study count for some countries\n",
    "country2ind = {}\n",
    "for indx,country in enumerate(pycountry.countries):\n",
    "    country2ind[country.name]=indx\n",
    "country_name_count = zip(country_names,country_counter)\n",
    "country_name_count = sorted(country_name_count,key = lambda(item):item[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df = pd.DataFrame(np.array(country_counter),country_names)\n",
    "df = df[df[0]>1]\n",
    "ax = df.plot(kind='bar',legend=False,title='Occurance of Country names')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between country vs average sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_occurence_ct(text,country):\n",
    "    name = country.name\n",
    "    counter = 0\n",
    "    global countries_new_name\n",
    "    if name in countries_new_name.keys():\n",
    "        name = countries_new_name[name]\n",
    "    name = '\\\\b'+name # Ensuring substring start with country name. eg.Indian, Syrian, India\n",
    "    all_matches = re.finditer(name,text,re.IGNORECASE)\n",
    "    n_occurrences = len(list(all_matches))\n",
    "    counter += n_occurrences\n",
    "    abbr_to_ignore = [\"RE\", \"FM\", \"TV\", \"AL\", \"AQ\", \"LA\", \"BEN\"]\n",
    "    if country.alpha2 not in abbr_to_ignore:\n",
    "        name = '\\\\b'+country.alpha2+'\\\\b'\n",
    "        all_matches = re.finditer(name,text)\n",
    "        n_occurrences = len(list(all_matches))\n",
    "        counter += n_occurrences\n",
    "    if country.alpha3 not in abbr_to_ignore:\n",
    "        name = '\\\\b'+country.alpha3+'\\\\b'\n",
    "        all_matches = re.finditer(name,text)\n",
    "        n_occurrences = len(list(all_matches))\n",
    "        counter += n_occurrences\n",
    "    return counter\n",
    "def get_country_sentiment(country):\n",
    "    global sentiments\n",
    "    global emails\n",
    "    res_pos,res_neg = 0,0\n",
    "    counts = emails.ExtractedBodyText.apply(get_occurence_ct,args=(country,))\n",
    "    sentiments_pos = [tpl[1] for tpl in sentiments.values]\n",
    "    sentiments_neg = [tpl[2] for tpl in sentiments.values]\n",
    "    if sum(counts.values)>0:\n",
    "        res_pos = sum([ct*pos for ct,pos in zip(counts.values,sentiments_pos)])/sum(counts.values)\n",
    "        res_neg = sum([ct*neg for ct,neg in zip(counts.values,sentiments_neg)])/sum(counts.values)\n",
    "    return res_pos,res_neg\n",
    "countries = pd.Series([country for country in pycountry.countries])\n",
    "senti = countries.apply(get_country_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_pos = np.array([i[0] for i in senti])\n",
    "df = pd.DataFrame(senti_pos,country_names)\n",
    "df = df[df[0]>0.1]\n",
    "ax = df.plot(kind='bar',legend=False,title='Hillary\\'s Sentiment about countries')\n",
    "ax.set_ylabel('postivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "senti_neg = np.array([i[1] for i in senti])\n",
    "df = pd.DataFrame(senti_neg,country_names)\n",
    "df = df[df[0]>0]\n",
    "ax = df.plot(kind='bar',legend=False,title='Hillary\\'s Sentiment about countries')\n",
    "ax.set_ylabel('negativity')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
